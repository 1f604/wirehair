	Wirehair FEC - Fast Forward Error Correction Codec

		Wirehair FEC produces a stream of error correction blocks from a
	data source.  When enough of these blocks are received, the original
	data can be recovered.

		Wirehair is an FEC codec used to improve reliability of data sent
	over a Binary Erasure Channel (BEC) such as satellite Internet or WiFi.
	The data to transmit over the BEC is encoded into equal-sized blocks.
	When enough blocks are received at the other end of the channel, then
	the original data can be recovered.

		How many additional blocks are needed is random, though the number
	of additional blocks is low and does not vary based on the size of the
	data.  Typically one additional block is needed just 3% of the time.
	And two blocks are needed 0.1% of the time.

		Wirehair is released under the BSD license, which means that I ask
	only that if you use my software that in the binary distribution of your
	software you include the copyright notice in WIREHAIR.LICENSE and maybe
	it would be nice to say thank you in an Email. =)

-Christopher A Taylor (chris@slycog.com) http://slycog.com


News Update: 3/6/2012
+ Added heavy rows to improve code performance.
+ Completed most of the tuning.

Release TODO:
+ (!!) Resuming after an extra block is needed is currently broken.
+ First 256 elements need tables for tuning that are cooking right now.
+ Other elements need a small exception list so they will work every time.

After Release TODO:
+ Also optimize GE subdiagonal evaluation with a window for larger N.
+ Conjugate gradient method is being worked on by myself and Noah, maybe(?)
+ Provisional patent to keep it free for everyone and safe from evil corporations.
+ Formal documentation, white papers, conference abstract.
+ SIMD memxor() and prefetch is being worked on by Nate Tinkler.
+ Multi-threading is being worked on by Brandon McKune.
+ Java port is being worked on by Steve Sheppard.
+ C and Objective-C ports are being worked on by me.
+ C# port?
+ R port?

Benchmark results (before tuning):

These are out of date.  Right now, the smaller sizes have much less
throughput than before, due to the much stronger code being used.
The larger sizes are about the same as listed below because the
stronger code is a constant-time algorithm and scales very well.

Single-threaded performance on an Intel Xeon @ 3.07 GHz.
Test software compiled in 64-bit Release mode in MSVC.
3/1/2012, before tuning for different values of N.

Loss rate = 50%
Block size = 1500 Bytes

N      Size (MB)  Enc. Dec. (MB/s)
64,000   96       185  170  <- This is the largest supported N
32,768   49       220  190
16,384   24       266  235
 8,192   12       310  275
 4,096    6       345  290
 2,048    3       400  370
 1,024  1.5       400  370
   512  .768      400  430
   256  .384      450  500
   128  .192      430  385
    64  .096      440  530
    32  .048      455  550
    16  .027      400  600
     8  .012      500  600
     6  .009      420  650  <- This is the smallest supported N

I also tried N = 2048 for block size = 1 MB, or >2 GB of data and it was able to encode and decode at over 300 MB/s.  Protecting 2 GB of data at that level of performance with just a few MB of overhead is not bad.
